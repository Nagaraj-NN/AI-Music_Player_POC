# AI Music Mood Detection - Monorepo

A cross-platform music recommendation app that detects emotions from voice using AI and suggests music via Spotify API.

## üéØ Project Overview

This monorepo contains:
- **ML Backend**: Hybrid CNN (Wav2Vec2 + custom CNNs) for emotion detection from audio
- **Mobile App**: React Native app for iOS, Android, and Web (coming soon)
- **Shared Libraries**: Type-safe constants and API contracts shared between backend and frontend

### Key Features
- **Emotion Detection**: 8-class emotion recognition (neutral, calm, happy, sad, angry, fearful, disgust, surprised)
- **Lightweight ML**: Optimized for 16GB RAM laptops with <100ms inference
- **Cross-Platform**: Single codebase for mobile (React Native) and web
- **Type-Safe Integration**: Shared TypeScript/Python types ensure consistency
- **Spotify Integration**: Maps emotions to music moods for personalized recommendations

## üìÅ Monorepo Structure

```
AI-Music_Player_POC/
‚îú‚îÄ‚îÄ ml-backend/                   # Python ML backend (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ src/                     # Core CNN logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cnn_model.py         # Hybrid CNN (Wav2Vec2 + 1D/2D CNN)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py       # HuggingFace dataset loader
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # Audio processing utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference.py         # Standalone inference
‚îÇ   ‚îú‚îÄ‚îÄ training/                # Model training scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py             # Training pipeline with Wav2Vec2
‚îÇ   ‚îú‚îÄ‚îÄ api/                     # FastAPI endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app.py               # Emotion prediction API
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Saved model weights
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Datasets and configs
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ INSTALL.md              # ML backend setup guide
‚îÇ
‚îú‚îÄ‚îÄ mobile-app/                   # React Native app (iOS/Android/Web)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep                 # Placeholder (app coming soon)
‚îÇ
‚îú‚îÄ‚îÄ shared/                       # Shared code between backend and frontend
‚îÇ   ‚îú‚îÄ‚îÄ constants/               # Shared constants
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotions.py          # Python version (ML backend)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ emotions.ts          # TypeScript version (mobile app)
‚îÇ   ‚îú‚îÄ‚îÄ types/                   # Type definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Python Pydantic models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-contracts.ts     # TypeScript interfaces
‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Shared libraries documentation
‚îÇ
‚îú‚îÄ‚îÄ docs/                         # Project documentation
‚îÇ   ‚îî‚îÄ‚îÄ 3-Week Plan...           # Development roadmap
‚îÇ
‚îî‚îÄ‚îÄ README.md                     # This file
```

## üèóÔ∏è ML Architecture

### Hybrid CNN with Pre-trained Wav2Vec2
The model leverages **pre-trained Wav2Vec2** as a frozen feature extractor and adds custom CNN classifiers:

**Architecture Flow:**
```
Raw Audio (16kHz, 3s)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pre-trained Wav2Vec2 (FROZEN)              ‚îÇ
‚îÇ  - Trained on 960h LibriSpeech             ‚îÇ
‚îÇ  - 768-dim contextual speech features       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1D-CNN Branch   ‚îÇ    2D-CNN Branch         ‚îÇ
‚îÇ  (Wav2Vec2       ‚îÇ    (Mel Spectrogram)     ‚îÇ
‚îÇ   features)      ‚îÇ                          ‚îÇ
‚îÇ  - Conv1D layers ‚îÇ    - Conv2D layers       ‚îÇ
‚îÇ  - 128‚Üí64 dims   ‚îÇ    - 32‚Üí64‚Üí128 filters   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì                      ‚Üì
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
    Feature Fusion (Concat)
               ‚Üì
    Dense Layers (256‚Üí128‚Üí8)
               ‚Üì
    8 Emotion Classes
```

### Why Wav2Vec2 + Custom CNN?

1. **Pre-trained Wav2Vec2** (Frozen)
   - Provides rich speech representations learned from 960 hours of audio
   - Captures phonetic, prosodic, and acoustic features
   - No training required - used as feature extractor only
   - Output: 768-dimensional contextual features

2. **1D-CNN Branch** (Trainable)
   - Processes Wav2Vec2 features temporally
   - Learns emotion-specific patterns from pre-trained representations
   - Lightweight: Only ~50K parameters to train

3. **2D-CNN Branch** (Trainable)
   - Processes Mel spectrogram for complementary frequency features
   - Captures harmonic and spectral patterns
   - Multi-layer convolution for spatial patterns

4. **Feature Fusion**
   - Concatenates features from both branches
   - Dense layers for final emotion classification
   - Dropout for regularization

### Emotion ‚Üí Music Mood Mapping
- **Neutral/Calm** ‚Üí Chill/Relaxing playlists
- **Happy** ‚Üí Upbeat music
- **Sad/Fearful** ‚Üí Uplifting/Calming music
- **Angry** ‚Üí Energetic music
- **Surprised** ‚Üí Exciting music

## üöÄ Getting Started

### Prerequisites
- **ML Backend**: Python 3.8+, 16GB RAM, CUDA GPU (optional)
- **Mobile App**: Node.js 18+, React Native CLI (coming soon)

### ML Backend Setup

1. Clone the repository:
```bash
git clone https://github.com/Nagaraj-NN/AI-Music_Player_POC.git
cd AI-Music_Player_POC
```

2. Install Python dependencies:
```bash
cd ml-backend
pip install -r requirements.txt
```

3. Train the model:
```bash
python training/train.py
```

**Training outputs:**
- `models/best_emotion_cnn.pth` - Best model weights (~200KB)
- `models/emotion_cnn.onnx` - ONNX export for production

**Expected accuracy:** 85-92% on validation set

### Running the ML API

#### Option 1: Standalone Inference
```bash
cd ml-backend
python src/inference.py path/to/audio.wav
```

#### Option 2: API Service
Start the FastAPI server:
```bash
cd ml-backend/api
python app.py
```

API will be available at `http://localhost:8000`

**API Endpoints:**
- `GET /` - Health check
- `POST /predict-emotion/` - Upload audio file for emotion prediction
- `POST /predict-realtime/` - Real-time prediction from audio buffer
- `GET /emotions/` - List supported emotions
- `GET /music-moods/` - Get emotion-to-mood mapping

### Mobile App Setup (Coming Soon)

```bash
cd mobile-app
npm install
npm run start     # Start Metro bundler
npm run android   # Run on Android
npm run ios       # Run on iOS
npm run web       # Run web version
```

### Example API Usage

```python
import requests

# Upload audio file
with open("voice_sample.wav", "rb") as f:
    response = requests.post(
        "http://localhost:8000/predict-emotion/",
        files={"file": f}
    )

result = response.json()
print(f"Emotion: {result['emotion']}")
print(f"Confidence: {result['confidence']}")
print(f"Music Mood: {result['music_mood']}")
```

## üìä Model Performance

| Metric | Target | Notes |
|--------|--------|-------|
| Accuracy | 85-92% | Wav2Vec2 features boost performance |
| Inference Latency | <100ms | Frozen Wav2Vec2 + lightweight CNN |
| Model Size | ~15MB | Only custom CNN weights saved |
| Wav2Vec2 Size | ~360MB | Downloaded once, cached by HuggingFace |
| RAM Usage | <4GB | Optimized for 16GB laptops |
| Training Time | 2-4 hours | On 5000 samples with CPU/GPU |

## üõ†Ô∏è Development Roadmap

### ‚úÖ Week 1: CNN Development with Wav2Vec2
- [x] HuggingFace dataset loader (zero prep)
- [x] Pre-trained Wav2Vec2 integration (frozen)
- [x] 1D-CNN on Wav2Vec2 features
- [x] 2D-CNN on Mel spectrograms
- [x] Hybrid CNN fusion architecture
- [x] Model training pipeline
- [x] ONNX export capability

### üìã Week 2: Real-time Pipeline
- [ ] PyAudio integration for mic capture
- [ ] 3-second audio buffering
- [ ] Real-time inference (<100ms)
- [ ] Confidence filtering (>70%)
- [ ] Streamlit UI prototype

### üìã Week 3: Spotify Integration
- [ ] Spotify API integration
- [ ] Emotion-to-playlist mapping
- [ ] Auto-play functionality
- [ ] Docker deployment
- [ ] Final demo and documentation

## üîß Configuration

### Model Hyperparameters
- **Sample Rate**: 16kHz
- **Audio Duration**: 3 seconds
- **Batch Size**: 32
- **Learning Rate**: 0.001
- **Epochs**: 20
- **Optimizer**: Adam
- **Scheduler**: ReduceLROnPlateau

### Audio Processing
- **Mel Spectrogram**: 64 mel bins, 1024 FFT, 512 hop length
- **Normalization**: [-1, 1] range
- **Augmentation**: Noise, gain, time shift

## üì¶ Dependencies

Key packages (see `requirements.txt` for full list):
- `torch` - Deep learning framework
- `torchaudio` - Audio processing
- `transformers` - HuggingFace Wav2Vec2 models
- `datasets` - HuggingFace datasets (zero prep)
- `librosa` - Audio feature extraction
- `fastapi` - API framework
- `uvicorn` - ASGI server
- `spotipy` - Spotify API client (Week 3)

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## üìù License

This project is for POC/educational purposes.

## üôè Acknowledgments

- HuggingFace for emotion recognition datasets
- PyTorch team for the framework
- FastAPI for the excellent API framework

## üìß Contact

**Nagaraj Nune**
- GitHub: [@Nagaraj-NN](https://github.com/Nagaraj-NN)

---

**Built with ‚ù§Ô∏è for AI-powered music experiences**
