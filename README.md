# AI Music Mood Detection POC

An AI-powered music recommendation app that detects emotions from voice frequency using a lightweight Hybrid CNN and suggests music based on detected mood.

## üéØ Project Overview

This project uses a **Hybrid CNN** (1D-CNN + 2D-CNN) to detect emotions from voice audio and maps them to appropriate music moods for personalized music recommendations via Spotify.

### Key Features
- **Emotion Detection**: 8-class emotion recognition (neutral, calm, happy, sad, angry, fearful, disgust, surprised)
- **Hybrid CNN Architecture**: Combines raw waveform (1D-CNN) and Mel spectrogram (2D-CNN) features
- **Lightweight**: Optimized for 16GB RAM laptops
- **Real-time**: <100ms inference latency for 3-second audio clips
- **API-First**: FastAPI service for easy integration with apps
- **HuggingFace Integration**: Zero data prep using pre-existing emotion datasets

## üìÅ Project Structure

```
AI-Music_Player_POC/
‚îú‚îÄ‚îÄ src/                          # Core CNN logic (separate from training)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ cnn_model.py             # Hybrid CNN (Wav2Vec2 + 1D/2D CNN)
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py           # HuggingFace dataset loader (zero prep)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                 # Audio processing utilities
‚îÇ   ‚îî‚îÄ‚îÄ inference.py             # Standalone inference module
‚îÇ
‚îú‚îÄ‚îÄ training/                     # Training scripts (isolated from core logic)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Package initialization
‚îÇ   ‚îî‚îÄ‚îÄ train.py                 # Model training pipeline with Wav2Vec2
‚îÇ
‚îú‚îÄ‚îÄ api/                          # API service layer (exposes model to app)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Package initialization
‚îÇ   ‚îî‚îÄ‚îÄ app.py                   # FastAPI endpoints for emotion prediction
‚îÇ
‚îú‚îÄ‚îÄ models/                       # Saved model weights (generated after training)
‚îÇ   ‚îú‚îÄ‚îÄ best_emotion_cnn.pth     # Trained CNN weights (only custom layers)
‚îÇ   ‚îî‚îÄ‚îÄ emotion_cnn.onnx         # ONNX production model
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Sample audio data and datasets
‚îÇ   ‚îú‚îÄ‚îÄ README.md                # Dataset documentation and loading guide
‚îÇ   ‚îú‚îÄ‚îÄ dataset_config.yaml      # Dataset configuration (5K samples)
‚îÇ   ‚îî‚îÄ‚îÄ cache/                   # HuggingFace dataset cache (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies (optimized for 16GB RAM)
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üèóÔ∏è Architecture

### Hybrid CNN with Pre-trained Wav2Vec2
The model leverages **pre-trained Wav2Vec2** as a frozen feature extractor and adds custom CNN classifiers:

**Architecture Flow:**
```
Raw Audio (16kHz, 3s)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pre-trained Wav2Vec2 (FROZEN)              ‚îÇ
‚îÇ  - Trained on 960h LibriSpeech             ‚îÇ
‚îÇ  - 768-dim contextual speech features       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1D-CNN Branch   ‚îÇ    2D-CNN Branch         ‚îÇ
‚îÇ  (Wav2Vec2       ‚îÇ    (Mel Spectrogram)     ‚îÇ
‚îÇ   features)      ‚îÇ                          ‚îÇ
‚îÇ  - Conv1D layers ‚îÇ    - Conv2D layers       ‚îÇ
‚îÇ  - 128‚Üí64 dims   ‚îÇ    - 32‚Üí64‚Üí128 filters   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì                      ‚Üì
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
    Feature Fusion (Concat)
               ‚Üì
    Dense Layers (256‚Üí128‚Üí8)
               ‚Üì
    8 Emotion Classes
```

### Why Wav2Vec2 + Custom CNN?

1. **Pre-trained Wav2Vec2** (Frozen)
   - Provides rich speech representations learned from 960 hours of audio
   - Captures phonetic, prosodic, and acoustic features
   - No training required - used as feature extractor only
   - Output: 768-dimensional contextual features

2. **1D-CNN Branch** (Trainable)
   - Processes Wav2Vec2 features temporally
   - Learns emotion-specific patterns from pre-trained representations
   - Lightweight: Only ~50K parameters to train

3. **2D-CNN Branch** (Trainable)
   - Processes Mel spectrogram for complementary frequency features
   - Captures harmonic and spectral patterns
   - Multi-layer convolution for spatial patterns

4. **Feature Fusion**
   - Concatenates features from both branches
   - Dense layers for final emotion classification
   - Dropout for regularization

### Emotion ‚Üí Music Mood Mapping
- **Neutral/Calm** ‚Üí Chill/Relaxing playlists
- **Happy** ‚Üí Upbeat music
- **Sad/Fearful** ‚Üí Uplifting/Calming music
- **Angry** ‚Üí Energetic music
- **Surprised** ‚Üí Exciting music

## üöÄ Getting Started

### Prerequisites
- Python 3.8+
- 16GB RAM (minimum)
- CUDA-capable GPU (optional, for faster training)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/Nagaraj-NN/AI-Music_Player_POC.git
cd AI-Music_Player_POC
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Training the Model

Train the Hybrid CNN on HuggingFace emotion datasets:

```bash
python training/train.py
```

**Training outputs:**
- `models/best_emotion_cnn.pth` - Best model weights
- `models/emotion_cnn.onnx` - ONNX export for production

**Expected accuracy:** 85-92% on validation set (with full dataset)

### Running Inference

#### Option 1: Standalone Inference
```bash
python src/inference.py path/to/audio.wav
```

#### Option 2: API Service
Start the FastAPI server:
```bash
cd api
python app.py
```

API will be available at `http://localhost:8000`

**API Endpoints:**
- `GET /` - Health check
- `POST /predict-emotion/` - Upload audio file for emotion prediction
- `POST /predict-realtime/` - Real-time prediction from audio buffer
- `GET /emotions/` - List supported emotions
- `GET /music-moods/` - Get emotion-to-mood mapping

### Example API Usage

```python
import requests

# Upload audio file
with open("voice_sample.wav", "rb") as f:
    response = requests.post(
        "http://localhost:8000/predict-emotion/",
        files={"file": f}
    )

result = response.json()
print(f"Emotion: {result['emotion']}")
print(f"Confidence: {result['confidence']}")
print(f"Music Mood: {result['music_mood']}")
```

## üìä Model Performance

| Metric | Target | Notes |
|--------|--------|-------|
| Accuracy | 85-92% | Wav2Vec2 features boost performance |
| Inference Latency | <100ms | Frozen Wav2Vec2 + lightweight CNN |
| Model Size | ~15MB | Only custom CNN weights saved |
| Wav2Vec2 Size | ~360MB | Downloaded once, cached by HuggingFace |
| RAM Usage | <4GB | Optimized for 16GB laptops |
| Training Time | 2-4 hours | On 5000 samples with CPU/GPU |

## üõ†Ô∏è Development Roadmap

### ‚úÖ Week 1: CNN Development with Wav2Vec2
- [x] HuggingFace dataset loader (zero prep)
- [x] Pre-trained Wav2Vec2 integration (frozen)
- [x] 1D-CNN on Wav2Vec2 features
- [x] 2D-CNN on Mel spectrograms
- [x] Hybrid CNN fusion architecture
- [x] Model training pipeline
- [x] ONNX export capability

### üìã Week 2: Real-time Pipeline
- [ ] PyAudio integration for mic capture
- [ ] 3-second audio buffering
- [ ] Real-time inference (<100ms)
- [ ] Confidence filtering (>70%)
- [ ] Streamlit UI prototype

### üìã Week 3: Spotify Integration
- [ ] Spotify API integration
- [ ] Emotion-to-playlist mapping
- [ ] Auto-play functionality
- [ ] Docker deployment
- [ ] Final demo and documentation

## üîß Configuration

### Model Hyperparameters
- **Sample Rate**: 16kHz
- **Audio Duration**: 3 seconds
- **Batch Size**: 32
- **Learning Rate**: 0.001
- **Epochs**: 20
- **Optimizer**: Adam
- **Scheduler**: ReduceLROnPlateau

### Audio Processing
- **Mel Spectrogram**: 64 mel bins, 1024 FFT, 512 hop length
- **Normalization**: [-1, 1] range
- **Augmentation**: Noise, gain, time shift

## üì¶ Dependencies

Key packages (see `requirements.txt` for full list):
- `torch` - Deep learning framework
- `torchaudio` - Audio processing
- `transformers` - HuggingFace Wav2Vec2 models
- `datasets` - HuggingFace datasets (zero prep)
- `librosa` - Audio feature extraction
- `fastapi` - API framework
- `uvicorn` - ASGI server
- `spotipy` - Spotify API client (Week 3)

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## üìù License

This project is for POC/educational purposes.

## üôè Acknowledgments

- HuggingFace for emotion recognition datasets
- PyTorch team for the framework
- FastAPI for the excellent API framework

## üìß Contact

**Nagaraj Nune**
- GitHub: [@Nagaraj-NN](https://github.com/Nagaraj-NN)

---

**Built with ‚ù§Ô∏è for AI-powered music experiences**
