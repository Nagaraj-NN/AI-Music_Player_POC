# ML Backend - Emotion Detection API

Machine learning backend for emotion detection from voice audio using Hybrid CNN with pre-trained Wav2Vec2.

## ğŸ“ Structure

```
ml-backend/
â”œâ”€â”€ src/                     # Core CNN logic
â”‚   â”œâ”€â”€ cnn_model.py        # Hybrid CNN (Wav2Vec2 + 1D/2D CNN)
â”‚   â”œâ”€â”€ data_loader.py      # HuggingFace dataset loader
â”‚   â”œâ”€â”€ utils.py            # Audio processing utilities
â”‚   â””â”€â”€ inference.py        # Standalone inference
â”œâ”€â”€ training/               # Training scripts
â”‚   â””â”€â”€ train.py           # Model training pipeline
â”œâ”€â”€ api/                    # FastAPI endpoints
â”‚   â””â”€â”€ app.py             # Emotion prediction API
â”œâ”€â”€ models/                 # Saved model weights
â”œâ”€â”€ data/                   # Datasets and configs
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ INSTALL.md             # Detailed setup guide
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
cd ml-backend
pip install -r requirements.txt
```

### 2. Train the Model
```bash
python training/train.py
```

### 3. Start API Server
```bash
cd api
python app.py
```

API available at `http://localhost:8000`

## ğŸ“Š Model Details

- **Architecture**: Hybrid CNN (Wav2Vec2 + custom CNNs)
- **Pre-trained Model**: facebook/wav2vec2-base (frozen, 360MB)
- **Trainable Params**: ~50K (only custom CNN layers)
- **Emotions**: 8 classes (neutral, calm, happy, sad, angry, fearful, disgust, surprised)
- **Accuracy**: 85-92% on validation set
- **Inference**: <100ms for 3-second audio
- **Memory**: Peak 4GB training, 2GB inference

## ğŸ”Œ API Endpoints

### Health Check
```bash
GET http://localhost:8000/
```

### Predict Emotion
```bash
POST http://localhost:8000/predict-emotion/
Content-Type: multipart/form-data
file: audio.wav

Response:
{
  "emotion": "happy",
  "confidence": 0.9234,
  "music_mood": "upbeat",
  "all_probabilities": {...},
  "status": "success"
}
```

### Realtime Prediction
```bash
POST http://localhost:8000/predict-realtime/
Content-Type: multipart/form-data
audio_buffer: audio.wav

Response:
{
  "emotion": "calm",
  "confidence": 0.8567,
  "music_mood": "relaxing"
}
```

### List Emotions
```bash
GET http://localhost:8000/emotions/

Response:
{
  "emotions": ["neutral", "calm", "happy", "sad", "angry", "fearful", "disgust", "surprised"],
  "count": 8
}
```

### Get Music Mood Mapping
```bash
GET http://localhost:8000/music-moods/

Response:
{
  "neutral": "chill",
  "calm": "relaxing",
  "happy": "upbeat",
  ...
}
```

## ğŸ“¦ Dependencies

Key packages:
- `torch==2.1.0` (CPU-only for 16GB RAM optimization)
- `transformers==4.33.0` (Wav2Vec2 integration)
- `datasets` (HuggingFace dataset loading)
- `fastapi==0.103.1` (API framework)
- `librosa==0.10.1` (Audio processing)
- `torchaudio==2.1.0` (Audio utilities)

See `requirements.txt` for full list.

## ğŸ”§ Configuration

### Dataset Configuration
Edit `data/dataset_config.yaml`:
- `max_samples`: Number of training samples (default: 5000 for 16GB RAM)
- `batch_size`: Training batch size (default: 8)
- `train_split`: Train/validation split ratio (default: 0.8)

### Model Configuration
Edit constants in `training/train.py`:
- `BATCH_SIZE`: Batch size for training (8 for 16GB RAM)
- `NUM_EPOCHS`: Training epochs (default: 20)
- `WAV2VEC_MODEL`: Pre-trained model name (default: facebook/wav2vec2-base)

## ğŸ§ª Testing

### Test Inference
```bash
cd ml-backend
python src/inference.py path/to/test_audio.wav
```

### Test API
```bash
curl -X POST "http://localhost:8000/predict-emotion/" \
  -F "file=@test_audio.wav"
```

## ğŸ“š Additional Documentation

- See `INSTALL.md` for detailed installation and troubleshooting
- See `data/README.md` for dataset information
- See `../shared/README.md` for shared constants/types

## ğŸ¤ Integration with Mobile App

The ML backend uses shared constants from `../shared/` folder:
- `shared/constants/emotions.py` - Emotion labels and mappings
- `shared/types/models.py` - Pydantic models for API validation

These are kept in sync with TypeScript versions in the mobile app.

## ğŸ’¡ Notes

- Run all Python commands from `ml-backend/` directory
- Model weights are saved to `models/best_emotion_cnn.pth` after training
- First run downloads Wav2Vec2 (~360MB) and caches it in `~/.cache/huggingface/`
- Confidence threshold is 0.70 - predictions below this show a warning
